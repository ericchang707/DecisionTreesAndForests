{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc27743",
   "metadata": {},
   "source": [
    "\n",
    "# CS 3600: Artificial Intelligence - Assignment 4 - Decision Trees and Forests\n",
    "\n",
    "\n",
    "## Setup\n",
    "Clone this repository:\n",
    "\n",
    "`git clone https://github.gatech.edu/omscs6601/assignment_4.git`\n",
    "\n",
    "\n",
    "You will be able to use numpy, math, time and collections. Counter for the assignment\n",
    "\n",
    "You will be able to use sklearn and graphviz for jupyter notebook visualization only\n",
    "\n",
    "No other external libraries are allowed for solving this problem.\n",
    "\n",
    "Please use the ai_env environment from previous assignments\n",
    "\n",
    "```\n",
    "conda activate ai_env\n",
    "```\n",
    "\n",
    "The supplementary testing notebooks use jupyter: visualize_tree and unit_testing. From your ai_env Terminal:\n",
    "\n",
    "```\n",
    "pip install graphviz==0.19.1\n",
    "or alternatively\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you have difficulty or errors on graphviz0.19.1 From your ai_env Terminal:\n",
    "```\n",
    "conda install -c conda-forge python-graphviz\n",
    "```\n",
    "which installs version 0.19.1 (compatible).\n",
    "\n",
    "Python 3.7 is recommended and has been tested.\n",
    "\n",
    "\n",
    "## Overview\n",
    "Machine learning offers a number of methods for classifying data into discrete categories, such as k-means clustering. Decision trees provide a structure for such categorization, based on a series of decisions that led to separate distinct outcomes. In this assignment, you will work with decision trees to perform binary classification according to some decision boundary. Your challenge is to build and to train decision trees capable of solving useful classification problems. You will learn first how to build decision trees, then how to effectively train them and finally how to test their performance.\n",
    "\n",
    "<p>\n",
    "<img src=\"./files/dt.png\" alt=\"Decision Trees\" width=\"700\" height=\"350\"/>\n",
    "\n",
    "\n",
    "## Submission and Due Date\n",
    "\n",
    "The deliverable for the assignment is a **_submission.py_** upload to Gradescope.\n",
    "\n",
    "* All functions to be completed in **_submission.py_**\n",
    "\n",
    "**Important**:\n",
    "Submissions to Gradescope are rate limited for this assignment. **You can submit two submissions every 60 minutes during the duration of the assignment**.\n",
    "\n",
    "In your Gradescope submission history, you can mark a certain submission as 'Active'. Please ensure this is your best submission.\n",
    "\n",
    "### The Files\n",
    "\n",
    "You will only have to edit and submit **_submission.py_**, but there are a number of notable other files:\n",
    "1. **_submission.py_**: Where you will build your decision tree, confusion matrix, performance metrics, forests, and do the vectorization warm up.\n",
    "2. **_decision_trees_submission_tests.py_**: Sample tests to validate your trees, learning, and vectorization locally.\n",
    "3. **_Visualize_tree.ipnb_**: Helper Notebook to help you understand decision trees of various sizes and complexity\n",
    "4. **_unit_testing.ipynb_**: Helper Notebook to run through tests sequentially along with the readme\n",
    "\n",
    "### Resources\n",
    "1. **_Udacity Videos_**: [Lecture 7 on Machine Learning](https://classroom.udacity.com/courses/ud954/lessons/6808838653/concepts/67917548570923)  \n",
    "2. **_Textbook:Artificial Intelligence Modern Approach_**\n",
    "    Chapter 18 Learning from Examples\n",
    "    Chapter 20 Learning Probabilistic Models\n",
    "3. **_Cross-validation_** (https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n",
    "4. **_K-Fold Cross-validation_** (https://tibshirani.su.domains/sta306bfiles/cvwrong.pdf)\n",
    "    \n",
    "### Decision Tree Datasets\n",
    "    \n",
    "#### NOTE: path to the dataset: './data/your_file_name.csv'\n",
    "\n",
    "1. **_part23_data.csv_**: 4 features, 1372 data points, binary classification (last column)\n",
    "2. **_challenge_train.csv_**:  30 features, 6636 datapoints, binary classification (first column)\n",
    "3. **_mod_complex_binary.csv_**: 7 features, 1400 examples, binary classification (last column)\n",
    "#### Warmup Data\n",
    "4. **_vectorize.csv_**: data used during the vectorization warmup for Assignment 4\n",
    "\n",
    "\n",
    "### Imports\n",
    "**NOTE:** We are only allowing four imports: numpy, math, collections.Counter and time. We will be checking to see if any other libraries are used. You are not allowed to use any outside libraries. Please remember that you should not change any function headers.\n",
    "\n",
    "\n",
    "# HOW TO USE THIS NOTEBOOK\n",
    "\n",
    "## This notebook is meant to help structure your coding for the assignment, all it does is align the relevant tests for each section in a convenient format. Code Changes should still be made in submission.py. This notebook *should* dynamically reload your file for each test (please let us know if it doesnt). You do not need to submit this notebook, nor do you need to use it if you prefer running the unit tests from the command line. Remember to read the unit tests to understand what you are passing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b28842",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting Up some utilities for testing:\n",
    "from __future__ import division\n",
    "\n",
    "import unittest\n",
    "import submission as dt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import decision_trees_submission_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc9f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_tester(test, case):\n",
    "    importlib.reload(dt)\n",
    "    importlib.reload(decision_trees_submission_tests)\n",
    "    if test == decision_trees_submission_tests.DecisionTreePart1Tests:\n",
    "        print(\"Running Decision Tree Part 1 Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.DecisionTreePart2Tests:\n",
    "        print(\"Running Decision Tree Part 2Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.DecisionTreePart3Tests:\n",
    "        print(\"Running Decision Tree Part 3 Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.VectorizationWarmUpTests:\n",
    "        print(\"Running Vectoriization Warmup Tests: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.NameTests:\n",
    "        print(\"Name Test: {}\".format(case))\n",
    "        \n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(test(case))\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1e345",
   "metadata": {},
   "source": [
    "### Part 0: Vectorization!\n",
    "_[10 pts]_\n",
    "\n",
    "* File to use: **_vectorize.csv_**\n",
    "\n",
    "Vectorization is a process that provides enormous performance increases when processing large amounts of data. Whether one is training a deep neural network on millions of images, building random forests over a large dataset, or utilizing other algorithms, machine learning makes _extensive_ use of vectorization. In python, the **numpy** package provides a programmer with the ability to use python-wrapped, low-level optimizations written in C, however, the technique may feel strange at first and requires some practice to use comfortably.\n",
    "\n",
    "The data management in Assignment 4 can benefit from familiarity with these techniques. Additionally, Assignment 5 has a vectorization requirement so that it can run within a reasonable time limit. This small section will hopefully introduce you to vectorization and some of the cool tricks you can use in python. We encourage you to use any numpy function out there (on good faith) to do the functions in the warmup section.\n",
    "\n",
    "For the three functions that we have, we are testing your code based on how fast it runs. It will need to beat the non-vectorized code to get full points.\n",
    "\n",
    "As a reminder, TAs will not help on this section. This section was created to help get you ready for this and other assignments; feel free to ask other students on Ed Discussion or use some training resources. (e.g. https://numpy.org/learn/).\n",
    "\n",
    "How grading works:\n",
    "1. We run the non-vectorized code and your vectorized code 500 times, as long as the average time of your vectorized code is less than the average time of the non-vectorized code, you will get the points (given that your answer is correct).\n",
    "\n",
    "#### Functions to complete in the `Vectorization` class:\n",
    "1. `vectorized_loops()`\n",
    "2. `vectorized_slice()`\n",
    "3. `vectorized_flatten()`\n",
    "4. `vectorized_glue()`\n",
    "5. `vectorized_mask()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0036a5f",
   "metadata": {},
   "source": [
    "1. `vectorized_loops()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dcd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_loops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_loops_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb1a19",
   "metadata": {},
   "source": [
    "2. `vectorized_slice()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53972930",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e991040",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_slice_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7bf0d",
   "metadata": {},
   "source": [
    "3. `vectorized_flatten()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_flatten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_flatten_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d176c5",
   "metadata": {},
   "source": [
    "4. `vectorized_glue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee759561",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c747ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_glue_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe585364",
   "metadata": {},
   "source": [
    "5. `vectorized_mask()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_mask_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2006b2",
   "metadata": {},
   "source": [
    "## The Assignment\n",
    "Classification is used widely in machine learning to figure out how to sort new data that comes through.  You will build, train and test decision tree models to perform basic classification tasks. Students should understand how decision trees and random forests work. This will help you develop an intuition for how and why accuracy differs for training and testing data based on different parameters.\n",
    "\n",
    "### Introduction\n",
    "For this assignment we're going to need an explicit way to make structured decisions. The `DecisionNode` class will be used to represent a decision node as some atomic choice in a binary decision graph. We would only use this implementation of the Decision Tree for this assignment and any other implementations will be checked against and denied credit.\n",
    "\n",
    "An object of type 'DecisionNode' can represent a\n",
    "\n",
    "  * decision node\n",
    "     - left: will point to less than or equal values of the split value, type DecisionNode, True evaluations\n",
    "     - right: will point to greater than values of the split value, type DecisionNode, False evaluations\n",
    "     - decision_function: evaluates an attribute's value and maps each vector to a descendant\n",
    "     - class_label: None\n",
    "  * leaf node\n",
    "     - left: None\n",
    "     - right: None\n",
    "     - decision_function: None\n",
    "     - class_label: A leaf node's class value\n",
    "  * Note that in this representation 'True' values for a decision take us to the left. This choice is arbitrary, but this is used in the hint below.\n",
    "\n",
    "\n",
    "### Part 1a: Building a Binary Tree by Hand\n",
    "_[15 Pts]_\n",
    "\n",
    "In `build_decision_tree()`, construct a tree of decision nodes by hand in order to classify the data below, i.e. map each datum **x** to a label **y**.  Your tests should use as few attributes as possible, break ties among tests with the same number of attributes by selecting the one that classifies the greatest number of examples correctly. If multiple tests have the same number of attributes and classify the same number of examples, then break the tie using attributes with lower index numbers (e.g. select **A1** over **A2**)\n",
    "<p>\n",
    "\n",
    "| Datum\t| A1  | A2  | A3  | A4  |  y  |\n",
    "| ----- | --- | --- | --- | --- | --- |\n",
    "| x1    |  1  |  0  |  0  |  0  |  1  |\n",
    "| x2    |  1  |  0  |  1  |  1  |  1  |\n",
    "| x3    |  0  |  1  |  0  |  0  |  1  |\n",
    "| x4    |  0  |  1  |  1  |  0  |  0  |\n",
    "| x5    |  1  |  1  |  0  |  1  |  1  |\n",
    "| x6    |  0  |  1  |  0  |  1  |  0  |\n",
    "| x7    |  0  |  0  |  1  |  1  |  1  |\n",
    "| x8    |  0  |  0  |  1  |  0  |  0  |\n",
    "\n",
    "#### Requirements:\n",
    "The total number of elements(nodes, leaves) in your tree should be < 10.\n",
    "\n",
    "\n",
    "#### Hints:\n",
    "To get started, it might help to **draw out the tree by hand** with each attribute representing a node.\n",
    "\n",
    "To create the decision function that will be passed to `DecisionNode`, you can create a lambda expression as follows:\n",
    "\n",
    "    func = lambda feature : feature[2] == 0\n",
    "\n",
    "This will choose the left node if the third attribute is 0.\n",
    "\n",
    "For example, a tree looks like this:\n",
    "\n",
    "> Lets say if A1==0 then class would be 1; else class would be 0. This node can be represented by:\n",
    "> <p>\n",
    "> <img src=\"./files/tree_example.png\" alt=\"Tree Example\"/>\n",
    "\n",
    "You would write your code like this:\n",
    "\n",
    "    decision_tree_root = DecisionNode(None, None, lambda a1: a1 == 0)\n",
    "    decision_tree_root.left = DecisionNode(None, None, None, 1)\n",
    "    decision_tree_root.right = DecisionNode(None, None, None, 0)\n",
    "\n",
    "    return decision_tree_root\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "1. `build_decision_tree()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8057569",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_hand_tree_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98371c",
   "metadata": {},
   "source": [
    "\n",
    "### Part 1b: Precision, Recall, Accuracy and Confusion Matrix\n",
    "_[12 pts]_\n",
    "\n",
    "Now that we have a decision tree, we're going to need some way to evaluate its performance. In most cases we would reserve a portion of the training data for evaluation, or use cross-validation. \n",
    "\n",
    "Your confusion matrix should be K x K, K = number of classes. In the binary case, K =2. Actual labels (true labels) of the dataset will be represented by the rows, and the predicted labels form the columns. Notice that the correct classifier predictions form the diagonal of the matrix. True positives are samples where the prediction matches the true label, false positives are samples that were predicted positive, but are actually negative. False negatives are samples that were predicted negative, but were actually positive, whereas true negatives were predicted negative and are negative. It will be very helpful to use the numpy diag (or diagonal) function in this part of the assignment. You will have to consider carefully by class what the diagonal value tells you, what its row tells you, what its column tells you, and what is left?\n",
    "\n",
    "    * Accuracy: Of all the examples, what percentage did my classifier predict correctly?\n",
    "    * Precision: How often is my classifier right when it makes a positive prediction?\n",
    "    * Recall: How often does my classifier recognize positive examples? \n",
    "    \n",
    "Fill out the methods to compute the confusion matrix, accuracy, precision and recall for your classifier output. classifier_output will be the labels that your classifier predicts, while the true_labels will be the true test labels. Helpful references:\n",
    "\n",
    "\n",
    "  * Wikipedia: (https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "  * Metrics for Multi-Class Classification: (https://arxiv.org/pdf/2008.05756.pdf)\n",
    "  * Performance Metrics for Activity Recognition Sec 5: (https://www.nist.gov/system/files/documents/el/isd/ks/Final_PerMIS_2006_Proceedings.pdf#page=143)\n",
    "\n",
    "\n",
    "\n",
    "If you want to calculate the example set above by hand, run the following code.\n",
    "\n",
    "    classifier_output = [decision_tree_root.decide(example) for example in examples]\n",
    "\n",
    "    p1_confusion_matrix = confusion_matrix(classifier_output, classes)\n",
    "    p1_accuracy = accuracy( classifier_output, classes )\n",
    "    p1_precision = precision(classifier_output, classes)\n",
    "    p1_recall = recall(classifier_output, classes)\n",
    "\n",
    "    print p1_confusion_matrix, p1_accuracy, p1_precision, p1_recall\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "1. `confusion_matrix()`\n",
    "2. `precision()`\n",
    "3. `recall()`\n",
    "4. `accuracy()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95acda",
   "metadata": {},
   "source": [
    "1. `confusion_matrix()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c24db",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73cfdda",
   "metadata": {},
   "source": [
    "2. `precision()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_precision_calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8b080",
   "metadata": {},
   "source": [
    "3. `recall()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_recall_calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f8f3d",
   "metadata": {},
   "source": [
    "4. `accuracy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a29066",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_accuracy_calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b576f6b",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2a: Decision Tree Learning\n",
    "_[12 pts]_\n",
    "\n",
    "The first step in order to learn how best to create a decision tree, we need to know how well we are splitting the data. This is usually done by measuring the entropy of each split and using it to calculate information gain, but we'd like you to use GINI impurity instead of entropy for this assignment. We can do this by calculating the  `gini_impurity` and `gini_gain()` on the various splits.\n",
    "    \n",
    "The challenge will be to choose the best attribute at each decision with the lowest impurity. At each attribute we search for the best value to split on, the hypotheses are compared against what we currently know, because would we want to split if we learn nothing? Hints:\n",
    "\n",
    " * Gini impurity (https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
    " * [Slide deck](./files/Gini%20Impurity.png) for Gini Impurity.\n",
    " * Information gain (https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)\n",
    " * The Gini Gain follows a similar approach to information gain, replacing entropy with Gini Impurity.\n",
    " * Numpy helpful functions include advanced indexing, and filtering arrays with masks, slicing, stacking and concatenating\n",
    "<p>\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "1. `gini_impurity()`\n",
    "2. `gini_gain()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42b512",
   "metadata": {},
   "source": [
    "1. `gini_impurity()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b800f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_impurity_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e634e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_impurity_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_impurity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abd303",
   "metadata": {},
   "source": [
    "2. `gini_gain()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain_restaurant_patrons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75166ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain_restaurant_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e3e00",
   "metadata": {},
   "source": [
    "### Part 2b: Decision Tree Learning\n",
    "_[30 pts]_\n",
    "\n",
    "* File to use: **_part23_data.csv_**\n",
    "* Grading: average test accuracy over 10 rounds should be >= 70%\n",
    "\n",
    "As the size of our training set grows, it rapidly becomes impractical to build these trees by hand. We need a procedure to automagically construct these trees.\n",
    "\n",
    "To do list:\n",
    "\n",
    "      - Initialize the class with useful variables and assignments\n",
    "      - Fill out the __build_tree__ function\n",
    "      - Fill out the classify function\n",
    "      \n",
    "The  fit() member function will fit the data to the tree, using __build_tree__()\n",
    "\n",
    "For starters, let's consider the following algorithm (a variation of [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)) for the construction of a decision tree from a given set of examples:\n",
    "1. Check for base cases:\n",
    "   <!-- 1. If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "   2. If a specified depth limit is reached, return a leaf labeled with the most frequent class. -->\n",
    "      - If all input vectors have the same class, return a leaf node with the appropriate class label.\n",
    "      - If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "      - Splits producing 0, 1 length vectors\n",
    "      - Splits producing less or equivalent information\n",
    "      - Division by zero\n",
    "\n",
    "2. For each attribute alpha: evaluate the normalized gini gain gained by splitting on attribute `alpha`.\n",
    "3. Let `alpha_best` be the attribute with the highest normalized gini gain.\n",
    "4. Create a decision node that splits on `alpha_best`.\n",
    "5. Repeat on the sublists obtained by splitting on `alpha_best`, and add those nodes as children of this node\n",
    "6. When splitting a dataset and classes, they must stay synchronized, do not orphan or shift the indexes independently\n",
    "7. Use recursion to build your tree, by using the split lists, remember true goes left using decide\n",
    "8. The features are real numbers, you will need to split based on a threshold. Consider different approaches for what this threshold might be.\n",
    "\n",
    "First, in the `DecisionTree.__build_tree__()` method implement the above algorithm.\n",
    "Next, in `DecisionTree.classify()`, write a function to produce classifications for a list of features once your decision tree has been built using the `decide()` function.\n",
    "\n",
    "Some other helpful notes:\n",
    "1. Your features and classify should be in numpy arrays where if the dataset is (_m_ x _n_) then the features is (_m_ x _n_-1) and classify is (_m_ x _1_)\n",
    "2. These features are continuous features and you will need to split based on a threshold.\n",
    "\n",
    "How grading works in GradeScope:\n",
    "1. We load **_part23_data.csv_** and create our cross-validation training and test set with a `k=10` folds.  We use our own `generate_k_folds()` method.\n",
    "2. We classify the training data onto the three then fit the testing data onto the tree.\n",
    "3. We check the accuracy of your results versus the true results and we return the average of this over 10 iterations.\n",
    "\n",
    "#### Functions to complete in the `DecisionTree` class:\n",
    "1. `__build_tree__()`\n",
    "2. `classify()`\n",
    "\n",
    "Local Tests will Simply Check to make sure you can fit to 100% accuracy on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1311759",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_decision_tree_all_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7caf5",
   "metadata": {},
   "source": [
    "### Part 3: Random Forests\n",
    "_[20 pts]_\n",
    "\n",
    "* File to use: **_mod_complex_binary.csv_**\n",
    "* Allowed to write additional functions to improve your score\n",
    "* Grading: average test accuracy over 10 rounds should be >= 75%\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated as 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "1. For every tree we're going to build:\n",
    "   1. Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "   2. From the sample in the first step, choose attributes at random to learn on (in accordance with a provided attribute subsampling rate). (Without replacement)\n",
    "   3. Fit a decision tree to the subsample of data we've chosen (to a certain depth).\n",
    "\n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "Fill in `RandomForest.fit()` to fit the decision tree as we describe above, and fill in `RandomForest.classify()` to classify a given list of examples.\n",
    "\n",
    "Your features and classify should be in numpy arrays where if the dataset is (_m_ x _n_) then the features is (_m_ x _n_-1) and classify is (_n_ x _1_).\n",
    "\n",
    "To test, we will be using a forest with 200 trees, with a depth limit of 3, example subsample rate of 0.2 and attribute subsample rate of 0.3.\n",
    "\n",
    "How grading works:\n",
    "1. Similar to 2b but with the call to Random Forest.\n",
    "\n",
    "#### Functions to complete in the `RandomForest` class:\n",
    "1. `fit()`\n",
    "2. `classify()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart3Tests, 'test_binary_random_forest_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart3Tests, 'test_binary_random_forest_split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d1550",
   "metadata": {},
   "source": [
    "### Part 4: Return Your name!\n",
    "_[1 pts]_\n",
    "Return your name from the function `return_your_name()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.NameTests, 'test_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
